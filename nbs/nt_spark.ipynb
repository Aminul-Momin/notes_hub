{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Terminology\" data-toc-modified-id=\"Terminology-0.1\">Terminology</a></span><ul class=\"toc-item\"><li><span><a href=\"#SparkContext\" data-toc-modified-id=\"SparkContext-0.1.1\">SparkContext</a></span></li><li><span><a href=\"#SparkSession\" data-toc-modified-id=\"SparkSession-0.1.2\">SparkSession</a></span></li><li><span><a href=\"#Transformations\" data-toc-modified-id=\"Transformations-0.1.3\">Transformations</a></span></li><li><span><a href=\"#Actions\" data-toc-modified-id=\"Actions-0.1.4\">Actions</a></span></li><li><span><a href=\"#Spark-Configuration-Files\" data-toc-modified-id=\"Spark-Configuration-Files-0.1.5\">Spark Configuration Files</a></span></li></ul></li></ul></li><li><span><a href=\"#Spark-SQL:\" data-toc-modified-id=\"Spark-SQL:-1\">Spark SQL:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Creating-the-Spark-Entry-point-(SparkSession)\" data-toc-modified-id=\"Creating-the-Spark-Entry-point-(SparkSession)-1.0.1\">Creating the Spark Entry point (SparkSession)</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-1.0.2\">Data Preparation</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Spark 3.1.0](https://spark.apache.org/docs/3.0.1/index.html)\n",
    "- [Spark Python API](https://spark.apache.org/docs/latest/api/python/index.html#)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/reference/index.html#api-reference)\n",
    "- [Transformations and Actions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html#transformations-and-actions)\n",
    "- [Broadcast and Accumulator](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#broadcast-and-accumulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. When running SQL from within another programming language the results will be returned as a `Dataset`/`DataFrame`.\n",
    "- The `Dataset` API is available in Scala and Java. Python does not have the support for the Dataset API.\n",
    "- The `DataFrame` API is available in Scala, Java, Python, and R\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<center><img src=\"../assets/spark/cluster-overview.png\" width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "PySpark architecture is the underlying structure that enables the Python API for Apache Spark to distribute and process data across clusters efficiently. It comprises various components that work together to perform distributed data processing and analysis. Here's a detailed explanation of PySpark architecture:\n",
    "\n",
    "-   `Driver Program`: The driver program is the entry point of a PySpark application. It's the main Python script that defines the sequence of operations to be executed on the Spark cluster. The driver program interacts with the SparkContext to coordinate tasks and collect results.\n",
    "-   `SparkContext (Driver)`: The SparkContext is the central component of the driver program. It represents the connection to the Spark cluster and manages the allocation of resources, including memory and CPU, on the cluster. The SparkContext coordinates with the cluster manager to distribute tasks to worker nodes.\n",
    "-   `Cluster Manager`: The cluster manager (e.g., Apache Mesos, Hadoop YARN, or Spark's standalone cluster manager) is responsible for managing the resources of the cluster. It allocates tasks to available worker nodes based on available resources and scheduling policies.\n",
    "-   `Executor (Worker Nodes)`: Executors are worker nodes in the Spark cluster where actual computations take place. Each executor is responsible for running tasks on its allocated resources (CPU, memory). Executors store data in memory and on disk and communicate with the driver program and other executors.\n",
    "-   `Task`: A task is a unit of work that is executed on an executor. Tasks are based on the operations defined in the driver program and include transformations and actions on RDDs or DataFrames.\n",
    "-   `Job`: A job is a sequence of tasks that are launched as a result of an action being triggered in the driver program. Each action on an RDD or DataFrame triggers the execution of one or more jobs.\n",
    "-   `Distributed Data`: Data is distributed across partitions, with each partition residing on a different executor. Data processing tasks are executed in parallel on each partition, enabling efficient distributed computing.\n",
    "-   `Resilient Distributed Dataset (RDD)`: RDDs are the fundamental data structures in PySpark. They represent distributed collections of data, partitioned across the cluster. RDDs support transformations and actions, and they provide fault tolerance through lineage information.\n",
    "-   `DataFrame and Catalyst Optimizer`: DataFrames are higher-level abstractions that provide schema-aware, optimized querying capabilities. The Catalyst Optimizer optimizes DataFrame queries and execution plans for better performance.\n",
    "-   `Spark Core`: Spark Core is the foundation of the Spark architecture. It includes essential libraries and APIs for distributed task scheduling, memory management, and fault tolerance.\n",
    "-   `Cluster Communication`: The driver program communicates with the executors and the cluster manager to distribute tasks and collect results. Communication happens via serialized data, allowing data to be transferred efficiently across the network.\n",
    "-   `Storage Management`: Spark provides various storage levels for caching and persisting RDDs in memory or on disk. This helps reduce recomputation and improves performance.\n",
    "-   `Shuffle and Data Movement`: Shuffle is the process of redistributing data across partitions. This often occurs when data needs to be grouped or aggregated. Spark minimizes data shuffling to optimize performance.\n",
    "-   `Cluster Monitoring and Management`: Cluster monitoring tools track the status of the cluster, resource utilization, task execution progress, and failures. Monitoring helps administrators optimize cluster performance and troubleshoot issues.\n",
    "-   `Serialization`: Data serialization is used to convert data structures into a format that can be transmitted across the network or stored. PySpark uses serialization to send tasks, data, and results between driver and executor nodes.\n",
    "\n",
    "PySpark's architecture is designed to harness the power of distributed computing while abstracting away the complexities of managing clusters and parallelism. It allows developers to focus on defining high-level operations and let Spark handle the distribution, optimization, and fault tolerance of data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The SparkContext is the entry point to the underlying Spark engine and was the primary entry point to Spark before version 2.0. It is responsible for coordinating the resources and orchestrating the processing of data in a Spark application.\n",
    "\n",
    "The SparkContext is responsible for setting up internal services, including the scheduler, the task scheduler, and the cluster manager. It also sets up external services, such as Hadoop Distributed File System (HDFS), Apache Cassandra, and Apache HBase.\n",
    "\n",
    "In summary, the SparkContext is responsible for low-level programming of Spark, including job scheduling, task dispatching, and cluster management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The SparkSession, introduced in Spark 2.0, is a higher-level entry point to Spark that provides a single unified interface to interact with Spark. The SparkSession combines the functionality of the SparkContext, SQLContext, and HiveContext into a single object.\n",
    "\n",
    "The SparkSession provides a seamless integration with Spark SQL, which is the Spark module for structured data processing. It allows Spark applications to read and write data in various file formats, such as CSV, JSON, and Parquet, and execute SQL queries against it. The SparkSession also provides an API for working with datasets, which are a type-safe extension of the DataFrame API.\n",
    "\n",
    "In summary, the SparkSession provides a high-level API for working with Spark that combines the functionality of the SparkContext, SQLContext, and HiveContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- ***Selecting Columns**\n",
    "\n",
    "```python\n",
    "df.select(\"column1\", \"column2\")\n",
    "```\n",
    "\n",
    "- ***Filtering Rows**\n",
    "\n",
    "```python\n",
    "df.filter(df.column1 == \"value\")\n",
    "```\n",
    "\n",
    "- ***Grouping and Aggretting**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"column1\").agg({\"column2\": \"sum\"})\n",
    "```\n",
    "\n",
    "- ***Joining**\n",
    "\n",
    "```python\n",
    "joined_df = df1.join(df2, \"common_column\")\n",
    "```\n",
    "\n",
    "- ***Ordering**\n",
    "\n",
    "```python\n",
    "df.orderBy(\"column1\")\n",
    "```\n",
    "\n",
    "- **Windowing**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window = Window.partitionBy(\"column1\").orderBy(\"column2\")\n",
    "df.withColumn(\"row_number\", row_number().over(window))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Counting Rows**\n",
    "\n",
    "    ```python\n",
    "    df.count()\n",
    "    ```\n",
    "\n",
    "- **Collecting Data**\n",
    "\n",
    "    ```python\n",
    "    df.collect()\n",
    "    ```\n",
    "\n",
    "- **Writting Data**\n",
    "\n",
    "    ```python\n",
    "    # CSV\n",
    "    df.write.csv(\"path/to/output.csv\", header=True)\n",
    "\n",
    "    # JSON\n",
    "    df.write.json(\"path/to/output.json\")\n",
    "\n",
    "    # Parquet\n",
    "    df.write.parquet(\"path/to/output.parquet\")\n",
    "    ```\n",
    "\n",
    "- **Showing Data**\n",
    "\n",
    "    ```python\n",
    "    df.show()\n",
    "    ```\n",
    "\n",
    "- **Summarizing Data**\n",
    "\n",
    "    ```python\n",
    "    df.describe().show()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Spark Configuration Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- `$ code $SPARK_HOME/conf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import col, rand, desc, asc, sum as Fsum, udf     ## udf => UserDefinedFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Spark Entry point (SparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/25 18:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"auto\") \\\n",
    "        .getOrCreate()\n",
    "# Enable eager evaluation for better formatting of the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10000000)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark.conf.get(\"spark.sql.sources.bucketing.enabled\")\n",
    "\n",
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "\n",
    "# Disable Broadcast Join\n",
    "spark.conf.set(\"spar.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spar.sql.adaptive.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/Users/am/mydocs/Software_Development/notes_hub/nbs/spark-warehouse'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.get(\"spark.sql.parquet.filterPushDown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.getConf().getAll()\n",
    "# spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# CSV\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# JSON\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# Parquet\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ['DATA'] + '/IBM_Data_Analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! head -3 {DATA_DIR}/imports-85.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration',\n",
    "       'num-of-doors', 'body-style', 'drive-wheels', 'engine-location',\n",
    "       'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type',\n",
    "       'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke',\n",
    "       'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg',\n",
    "       'highway-mpg', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqldf = spark.read.csv(DATA_DIR + \"/imports-85.csv/\", header=False, inferSchema=True) # pyspark.sql.dataframe.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqldf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = sqldf.columns\n",
    "\n",
    "for old_col, new_col in zip(columns, column_names):\n",
    "    sqldf = sqldf.withColumnRenamed(old_col, new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creates a temporary view against which we can run SQL queries.\n",
    "# df = df.createOrReplaceTempView('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT * FROM user_log LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symboling: integer (nullable = true)\n",
      " |-- normalized-losses: string (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- fuel-type: string (nullable = true)\n",
      " |-- aspiration: string (nullable = true)\n",
      " |-- num-of-doors: string (nullable = true)\n",
      " |-- body-style: string (nullable = true)\n",
      " |-- drive-wheels: string (nullable = true)\n",
      " |-- engine-location: string (nullable = true)\n",
      " |-- wheel-base: double (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- width: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- curb-weight: integer (nullable = true)\n",
      " |-- engine-type: string (nullable = true)\n",
      " |-- num-of-cylinders: string (nullable = true)\n",
      " |-- engine-size: integer (nullable = true)\n",
      " |-- fuel-system: string (nullable = true)\n",
      " |-- bore: string (nullable = true)\n",
      " |-- stroke: string (nullable = true)\n",
      " |-- compression-ratio: double (nullable = true)\n",
      " |-- horsepower: string (nullable = true)\n",
      " |-- peak-rpm: string (nullable = true)\n",
      " |-- city-mpg: integer (nullable = true)\n",
      " |-- highway-mpg: integer (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqldf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqldf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(symboling=3, normalized-losses='?', make='alfa-romero', fuel-type='gas', aspiration='std', num-of-doors='two', body-style='convertible', drive-wheels='rwd', engine-location='front', wheel-base=88.6, length=168.8, width=64.1, height=48.8, curb-weight=2548, engine-type='dohc', num-of-cylinders='four', engine-size=130, fuel-system='mpfi', bore='3.47', stroke='2.68', compression-ratio=9.0, horsepower='111', peak-rpm='5000', city-mpg=21, highway-mpg=27, price='13495'),\n",
       " Row(symboling=3, normalized-losses='?', make='alfa-romero', fuel-type='gas', aspiration='std', num-of-doors='two', body-style='convertible', drive-wheels='rwd', engine-location='front', wheel-base=88.6, length=168.8, width=64.1, height=48.8, curb-weight=2548, engine-type='dohc', num-of-cylinders='four', engine-size=130, fuel-system='mpfi', bore='3.47', stroke='2.68', compression-ratio=9.0, horsepower='111', peak-rpm='5000', city-mpg=21, highway-mpg=27, price='16500')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqldf.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [75. Databricks | Pyspark | Performance Optimization - Bucketing](https://www.youtube.com/watch?v=fp0PN9Y9QiY)\n",
    "- [74. Databricks | Pyspark | Interview Question: Sort-Merge Join (SMJ)](https://www.youtube.com/watch?v=DFtvA5O58X4&list=PLgPb8HXOGtsQeiFz1y9dcLuXjRh8teQtw&index=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Broadcast Join](https://www.youtube.com/watch?v=4ck3KOfSCzE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `spark.sql.autoBroadtcastJoinThreshold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1, 10000, 1, 10).select(col(\"id\"), rand(10).alias(\"Attribute\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 1]\r",
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|         Attribute|\n",
      "+---+------------------+\n",
      "|  1|0.1709497137955568|\n",
      "|  2|0.8051143958005459|\n",
      "|  3|0.5775925576589018|\n",
      "|  4|0.9476047869880925|\n",
      "|  5|   0.2093704977577|\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.partitionBy(\"Gender\").parquet(output_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"parquet\").saveAsTable(\"non_bucketed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "! open file:/Users/am/mydocs/Software_Development/notes_hub/nbs/spark-warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
