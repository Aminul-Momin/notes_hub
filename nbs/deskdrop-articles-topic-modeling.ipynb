{"nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"file_extension": ".py", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "version": "3.6.1", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}}, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "1a34a238-eaec-44fa-86fd-b4bd0ce36e49", "_uuid": "7088fa3fe74795a8fe93bb49b5a5c19f5d46c2fd"}, "source": ["In this section, we apply [Topic Modeling ](https://en.wikipedia.org/wiki/Topic_model) to discover what are the main topics of the shared articles. The [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) method is used, implemented in [Gensim](https://radimrehurek.com/gensim/) framework."]}, {"cell_type": "code", "metadata": {"_cell_guid": "ecf14b29-4660-4adf-8521-45c5d2414ced", "_uuid": "573a228231375d8791139e18ef83e0dfd43e69e3", "collapsed": true}, "outputs": [], "source": ["import pandas as pd"], "execution_count": 15}, {"cell_type": "code", "metadata": {"_cell_guid": "084fb341-f1be-4ff3-8756-04fc08ea5d58", "_uuid": "97ef1349317843b9d4bcf5c84e52440572ca5f5f", "collapsed": true}, "outputs": [], "source": ["articles_df = pd.read_csv('../input/shared_articles.csv')\n", "articles_df.head(5)"], "execution_count": 16}, {"cell_type": "code", "metadata": {"_cell_guid": "d786886f-735f-4248-8a22-e0b0550b60d8", "_uuid": "c8cebaff3078195b7da96dd116ad03a4c87a7d0f", "collapsed": true}, "outputs": [], "source": ["#import logging\n", "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n", "\n", "from gensim import corpora, models, similarities\n", "from nltk.tokenize import RegexpTokenizer\n", "from nltk.corpus import stopwords"], "execution_count": 17}, {"cell_type": "code", "metadata": {"_cell_guid": "279e1e35-3f82-4491-9f40-a1ce6efc08a6", "_uuid": "9d921f1c4ce35863726a1872e89ae2890c4070dc", "collapsed": true}, "outputs": [], "source": ["#Filtering only English articles\n", "english_articles_df = articles_df[articles_df['lang'] == 'en']\n", "#Concatenating the articles titles and bodies\n", "english_articles_content = (english_articles_df['title'] + ' ' + english_articles_df['text']).tolist()"], "execution_count": 18}, {"cell_type": "code", "metadata": {"_cell_guid": "ffe25479-5de1-46a3-91db-4a1f51af61f5", "_uuid": "beba9f3e265327da3df978607cc39493aa5800a5", "collapsed": true}, "outputs": [], "source": ["#Loading a set of English stopwords\n", "english_stopset = set(stopwords.words('english')).union(\n", "                  {\"things\", \"that's\", \"something\", \"take\", \"don't\", \"may\", \"want\", \"you're\", \n", "                   \"set\", \"might\", \"says\", \"including\", \"lot\", \"much\", \"said\", \"know\", \n", "                   \"good\", \"step\", \"often\", \"going\", \"thing\", \"things\", \"think\",\n", "                  \"back\", \"actually\", \"better\", \"look\", \"find\", \"right\", \"example\", \n", "                   \"verb\", \"verbs\"})"], "execution_count": 29}, {"cell_type": "code", "metadata": {"_cell_guid": "14daa87e-fc47-4015-a478-d1fc573f659f", "_uuid": "81d421323b7dfe80fe011ec9abbb8d27a22e67de", "collapsed": true}, "outputs": [], "source": ["#Tokenizing words of articles\n", "tokenizer = RegexpTokenizer(r\"(?u)[\\b\\#a-zA-Z][\\w&-_]+\\b\")\n", "english_articles_tokens = list(map(lambda d: [token for token in tokenizer.tokenize(d.lower()) if token not in english_stopset], english_articles_content))"], "execution_count": 30}, {"cell_type": "code", "metadata": {"_cell_guid": "8212a6cf-60c1-4b63-88dc-cace387d42de", "_uuid": "91f7c27d2d1c310d8189812d8c102ba6286cfab7", "collapsed": true}, "outputs": [], "source": ["#Processing bigrams from unigrams (sets of two works frequently together in the corpus)\n", "bigram_transformer = models.Phrases(english_articles_tokens)\n", "english_articles_unigrams_bigrams_tokens = list(bigram_transformer[english_articles_tokens])"], "execution_count": 31}, {"cell_type": "code", "metadata": {"_cell_guid": "97982c97-9e00-4443-99bc-bc5582089c29", "_uuid": "a5ee0e9931395053473d6933e29addcec8b6e29e", "collapsed": true}, "outputs": [], "source": ["#Creating a dictionary and filtering out too rare and too common tokens\n", "english_dictionary = corpora.Dictionary(english_articles_unigrams_bigrams_tokens)\n", "english_dictionary.filter_extremes(no_below=5, no_above=0.4, keep_n=None)\n", "english_dictionary.compactify()\n", "print(english_dictionary)"], "execution_count": 32}, {"cell_type": "code", "metadata": {"_cell_guid": "cc268525-b60d-4a76-893a-473ad314f776", "_uuid": "1cb7df461ed20f68891610763942044bc4d4a87c", "collapsed": true}, "outputs": [], "source": ["#Processing Bag-of-Words (BoW) for each article\n", "english_articles_bow = [english_dictionary.doc2bow(doc) for doc in english_articles_unigrams_bigrams_tokens]"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "f569cb82-c07f-4b52-b3a0-132057a39a59", "_uuid": "08cc4580f3ffa04bb2a84870110ff12e9ac55723", "collapsed": true}, "outputs": [], "source": ["#Training the LDA topic model on English articles\n", "lda_model = models.LdaModel(english_articles_bow, id2word=english_dictionary, num_topics=30, passes=10, iterations=500)"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "8bb42388-0bbd-46f0-b152-a7fcd59d8df2", "_uuid": "6312df7e7cb21c36b563f8d75c7c1667447953a1", "collapsed": true}, "outputs": [], "source": ["#Processing the topics for each article\n", "english_articles_lda = lda_model[english_articles_bow]"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "2ca202a6-c782-45e7-a305-d68973007f99", "_uuid": "752fa6a23d81b350205ae860988465c5cee9c292", "collapsed": true}, "outputs": [], "source": ["def get_topics_top_words(model, max_words):\n", "    all_topics = model.show_topics(-1, max_words*2, False, False)\n", "    topics = []\n", "    for topic in all_topics:    \n", "        min_score_word = float(abs(topic[1][0][1])) / 2.\n", "        top_positive_words = list(map(lambda y: y[0].replace('_',' '), filter(lambda x: x[1] > min_score_word, topic[1])))[0:max_words]\n", "        topics.append('[' + ', '.join(top_positive_words) + ']')\n", "    return topics\n", "\n", "#Computing the main topic of each article\n", "topics_top_words = get_topics_top_words(lda_model, 5)"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "49bfade7-ff79-4388-b0d3-1fd938d3bd63", "_uuid": "51625d520ee60361c2b2f791719df34525092b72"}, "source": ["Below, we can see the discovered topics on English articles, sorted by the number of articles about the topics (popularity)."]}, {"cell_type": "code", "metadata": {"_cell_guid": "ef07edd4-b20f-4adb-bb67-f8e651ac3e75", "_uuid": "bc91453e01a1aade30d85ecd1bb75d28a2eb348e", "collapsed": true}, "outputs": [], "source": ["def get_main_topics(corpus_lda, topics_labels):\n", "    min_strength = (1.0 / float(len(topics_labels))) + 0.01\n", "    main_topics = map(lambda ts: sorted(ts, key=lambda t: -t[1])[0][0] if sorted(ts, key=lambda t: -t[1])[0][1] > min_strength else None, corpus_lda)\n", "    main_topics_labels = map(lambda x: topics_labels[x] if x != None else '', main_topics)\n", "    return list(main_topics_labels)\n", "\n", "#Return the discovered topics, sorted by popularity\n", "corpus_main_topics = get_main_topics(english_articles_lda, topics_top_words)\n", "\n", "main_topics_df = pd.DataFrame(corpus_main_topics, columns=['topic']).groupby('topic').size().sort_values(ascending=True).reset_index()\n", "main_topics_df.columns = ['topic','count']\n", "main_topics_df.sort_values('count', ascending=False)"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "f258eb61-92e6-40c2-9c73-ecd3ba71b3d4", "_uuid": "ce6280896cce1dce9dff63c904432854b6311956", "collapsed": true}, "outputs": [], "source": ["main_topics_df.plot(kind='barh', x='topic', y='count', figsize=(7,20), title='Main topics on shared English articles')"], "execution_count": null}], "nbformat_minor": 1}