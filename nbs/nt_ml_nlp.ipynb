{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f388e24d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Text-Processing\" data-toc-modified-id=\"Text-Processing-1\">Text Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Text-cleaning\" data-toc-modified-id=\"Text-cleaning-1.1\">Text cleaning</a></span></li><li><span><a href=\"#Normalization,-tokenization,-and-stop-words-removal\" data-toc-modified-id=\"Normalization,-tokenization,-and-stop-words-removal-1.2\">Normalization, tokenization, and stop words removal</a></span></li><li><span><a href=\"#Part-of-speech-tagging,-named-entity-recognition,-and-conference-resolution\" data-toc-modified-id=\"Part-of-speech-tagging,-named-entity-recognition,-and-conference-resolution-1.3\">Part of speech tagging, named entity recognition, and conference resolution</a></span></li><li><span><a href=\"#Stemming-and-Lemmatization\" data-toc-modified-id=\"Stemming-and-Lemmatization-1.4\">Stemming and Lemmatization</a></span></li></ul></li><li><span><a href=\"#Feature-Extraction\" data-toc-modified-id=\"Feature-Extraction-2\">Feature Extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bag-of-Words\" data-toc-modified-id=\"Bag-of-Words-2.1\">Bag of Words</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-2.2\">TF-IDF</a></span></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-2.3\">Word Embeddings</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-3\">Modeling</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7db1a",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "\n",
    "Take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction. The goal is to (1) extract plain text that is free of any source specific markup or constructs that are not relevant to your task, and (2) to reduce complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c4359",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265d801",
   "metadata": {},
   "source": [
    "- **Encoding check**\n",
    "\n",
    "  Check encoding in order to work with and display everything, and to avoid losing some information as we move and analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use chardet if cannot find encoding from formal means\n",
    "import chardet\n",
    "\n",
    "string_one = \"What kind of funny business is this? ü§®\"\n",
    "encoded_one = string_one.encode(\"utf-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a42a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'UTF-32', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Detect encoding\n",
    "print(chardet.detect(encoded_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ecf9584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What kind of funny business is this? ü§®'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then decode\n",
    "encoded_one.decode(\"utf-32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04e30e",
   "metadata": {},
   "source": [
    "- **Foreign language detection**\n",
    "\n",
    "  Know the language of each document, because the algorithms of quantitative approaches may not work with mixed-language corpora and we might not realize the results are rubbish for a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d527015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d894973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A library by Google\n",
    "import langdetect\n",
    "# Detect language\n",
    "langdetect.detect(string_one) # returns the ISO 639 code for the language\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ed2b7",
   "metadata": {},
   "source": [
    "- **Cleaning non-text entity**\n",
    "\n",
    "  Remove irrelevant items, such as HTML tags, url, metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4530e12",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-74407bad9256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example to fetch html data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fetch web page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.udacity.com/courses/all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "# Example to fetch html data\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Fetch web page\n",
    "response = requests.get(\"https://www.udacity.com/courses/all\")\n",
    "# Use BeautifulSoup to remove HTML tags\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "# Find all course summaries\n",
    "summaries = soup.find_all('div', class_=\"course-summary-card\")\n",
    "# Extract course title\n",
    "courses = []\n",
    "for summary in summaries:\n",
    "    courses.append(summary.select_one(\"h4\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73b272",
   "metadata": {},
   "source": [
    "### Normalization, tokenization, and stop words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdb919",
   "metadata": {},
   "source": [
    "- **Normalization**\n",
    "\n",
    "  Convert to all lowercase and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd4ebc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are the human people the ones who started the war  is ai a bad thing '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"Are the human people the ones who started the war? Is AI a bad thing?\"\n",
    "# Normalize text\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef5de1",
   "metadata": {},
   "source": [
    "- **Tokenization**\n",
    "\n",
    "  Split text into words or tokens.\n",
    "\n",
    "    <img src=\"../assets/nlp_resources/nlp_tokenization.png\" width=500>\n",
    "\n",
    "    **N-grams** is a group of n words appearing in sequence from a text.\n",
    "\n",
    "    <img src=\"../assets/nlp_resources/nlp_tokenization_ngrams.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53281275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize text\n",
    "words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy\n",
    "# !{sys.executable} -m pip install spacy\n",
    "# !{sys.executable} -m pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_md-3.0.0/en_coref_md-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ae19a50",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_coref_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0c2cd29277d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load the trained model of the English language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_coref_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Perform neural net magics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/fastai/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/fastai/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_coref_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the trained model of the English language\n",
    "nlp = spacy.load('en_coref_md')\n",
    "\n",
    "# Perform neural net magics\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize text\n",
    "words = [token for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509013e1",
   "metadata": {},
   "source": [
    "- **Stop Word Removal**\n",
    "\n",
    "  Remove words that are too common. **Stop words** are words that are useful for grammar and syntax, but don‚Äôt contain any important content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3677265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Remove stop words\n",
    "words = [word for word in words if word not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy\n",
    "import spacy\n",
    "# Load the trained model of the English language\n",
    "nlp = spacy.load('en_coref_md')\n",
    "# Remove stop words\n",
    "stoplisted = [token for token in doc if token.text not in nlp.Defaults.stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782548a7",
   "metadata": {},
   "source": [
    "### Part of speech tagging, named entity recognition, and conference resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36fe7e8",
   "metadata": {},
   "source": [
    "- **Part of Speech Tagging**\n",
    "\n",
    "  Identify different parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e8e59cc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-784d2d8243e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"I always lie down to tell a lie.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Tag parts of speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#Outputs [('I', 'PRP'), ('always', 'RB'), ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "from nltk import pos_tag\n",
    "text = \"I always lie down to tell a lie.\"\n",
    "# Tag parts of speech\n",
    "pos_tag(word_tokenize(text))\n",
    "#Outputs [('I', 'PRP'), ('always', 'RB'), ...]\n",
    "\n",
    "# Spacy\n",
    "# Keep only nouns and verbs\n",
    "noun_verbs = [token.text for token in stoplisted if token.pos_ == \"NOUN\" or token.pos_ == \"VERB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92dbb09",
   "metadata": {},
   "source": [
    "- **Named Entity Recognition**\n",
    "\n",
    "  Identify named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "text = \"Antonio joined Udacity Inc. in California.\"\n",
    "# Recognize named entities in a tagged sentence\n",
    "ne_chunk(pos_tag(word_tokenize(text))) # need to tokenize and tag beforehand\n",
    "\n",
    "# Outputs (S (PERSON Antonio/NNP) joined/VBD (ORGANIZATION Udacity/NNP Inc./NNP) ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50746b",
   "metadata": {},
   "source": [
    "- **Conference resolution**\n",
    "\n",
    "  Identify the original reference of pronouns (she, he, it, that, her, him, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60834f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy\n",
    "import spacy\n",
    "# Load the trained model of the English language\n",
    "nlp = spacy.load('en_coref_md')\n",
    "# Perform neural net magic\n",
    "test_doc = nlp(u\"My brother lives in a basement apartment. He really loves it.\")\n",
    "# Print the original reference of pronouns\n",
    "print(\"The coreferences are {}\".format(test_doc._.coref_clusters))\n",
    "print(\"The new sentence is \\\"{}\\\"\".format(test_doc._.coref_resolved))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba207008",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "- **Stemming and Lemmatization**\n",
    "\n",
    "  Convert words into their dictionary forms. A **lemma** is the core stem of a concept and/or word. **Lemmatization** is the process of stripping a word down to its root word so that different forms of the word are identified as having the same significance in text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b01121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "# Reduce words to their root forms\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "# Lemmatize verbs by specifying pos\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "# Spacy\n",
    "import spacy\n",
    "# Load the trained model of the English language\n",
    "nlp = spacy.load('en_coref_md')\n",
    "# Perform neural net magics\n",
    "doc = nlp(text)\n",
    "# Put in the lemmas, except for pronoun because they don't really have lemmas\n",
    "resolved_doc = [token.lemma_  if token.lemma_ != \"-PRON-\" else token for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670e9946",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Extract and produce feature representations that are appropriate for the type of NLP task you are trying to accomplish and the type of model you are planning to use.\n",
    "\n",
    "Feature extraction depends on the goal\n",
    "\n",
    "- Graphic model: symbolic nodes with relationship between them, e.g., WordNet\n",
    "- Statistic model at document level: spam detection or sentiment analysis, use a per document representation, e.g., bag-of-words or doc2vec\n",
    "- Statistic model at word or phrase level: text generation or machine translation, use a word level representation, e.g., word2vec or glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fab1b5",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64103302",
   "metadata": {},
   "source": [
    "- **Bag of words**\n",
    "\n",
    "  Document-term matrix<br>\n",
    "  <img src=\"../assets/nlp_resources/nlp_bow_eg0.png\" width=500>\n",
    "\n",
    "  To compare similarity between 2 documents\n",
    "\n",
    "  - Initialize a collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b36b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "corpus = [\"The first time you see The Second Renaissance it may look boring.\",\n",
    "        \"Look at it at least twice and definitely watch part 2.\",\n",
    "        \"It will change your view of the matrix.\",\n",
    "        \"Are the human people the ones who started the war?\",\n",
    "        \"Is AI a bad thing ?\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd087a33",
   "metadata": {},
   "source": [
    "  - Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164c45d",
   "metadata": {},
   "source": [
    "  - Use defined tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c93ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# initialize count vectorizer object\n",
    "vect = CountVectorizer(tokenizer=tokenize)\n",
    "# get counts of each token (word) in text data\n",
    "X = vect.fit_transform(corpus)\n",
    "# convert sparse matrix to numpy array to view\n",
    "X.toarray() # outputs array([[0, 1, ...], ..., [...]]\n",
    "# view token vocabulary and counts\n",
    "vect.vocabulary_ # outputs {'first': 6, 'time': 20, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6c6b3",
   "metadata": {},
   "source": [
    "- **Dot product**\n",
    "\n",
    "  - Formula: <br>\n",
    "  $$ a\\cdot b = \\sum a_0b_0 + a_1b_1 + ... $$\n",
    "  - Higher value means more similar\n",
    "  - (-) Only captures the portion of overlap and not affected by other values that are not in common, so very different pairs can have same value as identical pairs.\n",
    "  - (-) Treats every word as being equally important\n",
    "\n",
    "- **Cosine similarity**\n",
    "\n",
    "  - Formula: <br>\n",
    "  $$ cos(\\theta) = \\frac{a\\cdot b}{\\left \\| a \\right \\|\\cdot \\left \\| b \\right \\|} $$\n",
    "  - Values ranges between -1 (dissimilar) and 1 (most similar)\n",
    "  - (-) Treats every word as being equally important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810492a4",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58a200",
   "metadata": {},
   "source": [
    "Term Frequency ‚Äì Inverse Document Frequency (TF-IDF) is a measure intended to reflect the relative importance of a word in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7b869",
   "metadata": {},
   "source": [
    "- **TF (Term Frequency)**\n",
    "\n",
    "  - Commonly used definition: The count of the term _t_ in a document _d_, divided by the total number of terms in the document\n",
    "  \n",
    "  $$ \\frac{count(t, d)}{|d|} $$\n",
    "  \n",
    "    TF uses a ‚Äúbag of words‚Äù approach, where each document is represented by a ‚Äúbag of words‚Äù where grammar and word order are disregarded but multiplicity is kept.\n",
    "\n",
    "  - `CountVectorizer`: Convert a collection of text to vectors of token counts\n",
    "  - `HashingTF`: Hashing is the process of transforming data of arbitrary size to a fixed size. Each word is input in a hash function to give it a number. The same words result in the same hash ID. Each hash is counted returning the term frequency for each word\n",
    "\n",
    "      <img src=\"../assets/nlp_resources/nlp_tfidf_hashing.png\" width=300 height=260>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67473a75",
   "metadata": {},
   "source": [
    "- **IDF (Inverse Document Frequency)**\n",
    "\n",
    "  - Commonly used definition: Logarithm of the total number of documents in the colection _D_ divided by the number of documents where _t_ is present.\n",
    "  \n",
    "  $$ log \\frac{|D|}{|{d\\in D : t\\in d}|} $$\n",
    "   - aka\n",
    "   $$ log \\frac{total\\ number\\ of\\ documents}{number\\ of\\ documents\\ containing\\ target\\ word} $$\n",
    "  - The more documents that include the term the the lower the IDF score\n",
    "\n",
    "  - `IDFModel`: Scales each column based on feature vectors which decreases weights on words found in multiple documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7403f",
   "metadata": {},
   "source": [
    "- **TF-IDF**\n",
    "\n",
    "  - The product of TF and IDF\n",
    "    $$ tf(t, d)\\cdot idf(t, D) $$\n",
    "\n",
    "  - TF-IDF using bag of words results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5bfc3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-25f741f2c06e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# use counts from count vectorizer results to compute tf-idf values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# convert sparse matrix to numpy array to view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# initialize tf-idf transformer object\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "# use counts from count vectorizer results to compute tf-idf values\n",
    "tfidf = transformer.fit_transform(X)\n",
    "\n",
    "# convert sparse matrix to numpy array to view\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d31a2",
   "metadata": {},
   "source": [
    "  - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdc2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize tf-idf vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# compute bag of word counts and tf-idf values\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# convert sparse matrix to numpy array to view\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af257e8",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a655d",
   "metadata": {},
   "source": [
    "To control the size of the word representation, find an embedding for each word in some vector space and exhibit some desired properties. Embedding can be used for different purposes, e.g., find synonyms and analogies, identify concepts along which words are clustered, classifying words as +, -, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531d791",
   "metadata": {},
   "source": [
    "- **Word2Vec**\n",
    "\n",
    "  Word2vec is a combination of two techniques. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations.\n",
    "\n",
    "    <img src=\"../assets/nlp_resources/word2vec.png\" width=400>\n",
    "\n",
    "  - Continuous bag of words (CBOW)\n",
    "\n",
    "    - CBOW predicts the probability of a word given a context. A context may be a single word or a group of words.\n",
    "\n",
    "  - Skip-gram model\n",
    "\n",
    "    - Skip-gram model predicts the context given a word.\n",
    "\n",
    "      <img src=\"../assets/nlp_resources/word2vec_sgm.png\" width=400>\n",
    "\n",
    "  - Word2Vec properties\n",
    "\n",
    "    - (+) Robust, distributed representation\n",
    "    - (+) Vector size independent of vocab\n",
    "    - (+) Train one, store in lookup table\n",
    "    - (+) Deep learning ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d1b57",
   "metadata": {},
   "source": [
    "- **GloVe**\n",
    "\n",
    "  Global vectors for word representation (GloVe) learns vector representations for words by doing dimensionality reduction on the co-occurrence counts matrix. A large matrix of (context x words) co-occurrence information was constructed, i.e. for each \"word\" (the column), the probability to see this word in some \"context\" (the rows) in a large corpus.\n",
    "\n",
    "    <img src=\"../assets/nlp_resources/glove.png\" width=500>\n",
    "\n",
    "  - (+) Compared to Word2Vec, easier to parallelize the implementation which means it's easier to train over more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bdf0b7",
   "metadata": {},
   "source": [
    "- **Embeddings for deep learning**\n",
    "\n",
    "  Can use embedding lookup (transfer learning)\n",
    "\n",
    "    <img src=\"../assets/nlp_resources/embedding_deep_learning.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d1311d",
   "metadata": {},
   "source": [
    "- **t-SNE for visualizing word embedding**\n",
    "\n",
    "  t-distributed stochastic neighbor embedding (t-SNE) is a dimensionality reduction technique to map higher dimensional vectors into lower dimensional space, while maintaining relative distances between objects.s\n",
    "\n",
    "    <img src=\"../assets/nlp_resources/t-sne.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d8f1e",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Design a statistical or machine learning model, fit its parameters to training data, use an optimization procedure, and then use it to make predictions about unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d192d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
